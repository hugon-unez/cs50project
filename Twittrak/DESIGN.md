

We decided to implement our project with multiple functions that work together in our app.py file. This means that we can perform relatively complex operations while still maintaining a clean workspace and still be able to easily indentify errors. This becasue defining functions compartamentalizes our code, which can allow us to locate where errors are. In addition, defining functions moves some of the code to other files, which means that our main app.py file can be much cleaner visually. This allows us to better show and understand the flow of our code. 

The first data we pulled is Twitter Data. We use the Twitter API with tweepy wrapper in order to pull the inputted user's tweets from the last 7 days, if the username is found to be valid. Then, with the tweet data, we cleaned the response json into a formatted list of tweets in eastern time. This step was vital, as market data is in Eastern Time, and the Twitter time format was different than the financial data time. With the times of tweets, we pass the list of times into the financial data function. 

The financial data function took in the times of tweets as a parameter. With the finacial data api, we pulled a list of the times of all intraday prices for SPY in the last 30 days or so. Then, we found the intersection of the tweet times and financial reports using set functions. Once we had a list of times that was both a tweet and had financial data, we used this list to generate a 2 dimensional array that contained the stock price at open, stock price at close, and volume. This function returned that array to twitter data, which was returned to our data analysis function.

We decided to implement a machine learning model to make predictions in our project. This allows us to use more data to make more accurate predictions than we would be able to do otherwise. In order to accomplish this, we used sklearn in addition to pandas and numpy (This is a skill that Quin learned over the summer. We reviewed old code to refresh on the syntax). First, we take in the price and time data from the APIs that will be used to train our model. This includes the price 1 minute before a tweet is published, the price 5 minutes after, and the trading volume. In order to make the data compatible with our model, we next convert it to a pandas dataframe. The prices allow us to perform some simple math to calculate the change in price over the time interval and store these values in a new column. Next, we store the price changes in a separate list "y". We also create a dataframe "X" that only includes the opening price and volume data. 

Then, we supply X and Y to the sklearn decision tree regressor. We ask the model to try and predict the changes (stored in y) for the opening price and volume (stored in X) for each time the user has published a tweet. We made sure to set the random state to a constant value in order to have reproducable result if supplied with the same data. We also tuned the model with the maximum depth, and we found that 3 layers provides accurate results without taking up too much time or memory. Next, we fit the model to the data we obtained from the APIs. After that, we obtain the current price and volume (stored as "recent_data"). Next, we use the predict function with the recent data to predict the change in stock price if the user publishes a tweet now. 

We chose to generate and train a machine learning model each time the user requests a twitter account in order to be conscious of memory usage. It would be extremely impractical to create and store a model for every user on the twitter platform before the user even inputs anything. In addition, the number of tweets we are pulling for each user is not very large in the world of big data, which also means that runtimes will be relatively short. This means that it is more effective to pull data and train a prediction model after the user askes for a specific twitter user.

Since we had a beefy backend that required many hours of research, trial, and error, we made a relatively simple front end. Using html, css and python, we created a simple site with two pages. The main page simply takes input of a twitter username. If the name is invalid, the page is refreshed. We did not reprompt the user because we had spent so much time on the back end. If the name is valid, then our back end model runs and attempts to predict the closing price of SPY 5 minutes after a tweet is sent by the user. The user is shown a page with the results, wit 3 options for price increase, decrease, or no change. 
This was Twittrak. 
